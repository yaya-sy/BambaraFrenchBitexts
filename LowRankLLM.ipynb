{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOi0PqApGLEkbgpaLM3EBNw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yaya-sy/BambaraFrenchBitexts/blob/main/LowRankLLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/yaya-sy/speechscorer/blob/main/demo/speechscorer.ipynb)"
      ],
      "metadata": {
        "id": "vNyxPHRy-OYM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Low-Rank LLM for efficient continued pretraining, finetuning and inference\n",
        "\n",
        "ILLUSTRATION [TODO]\n",
        "\n"
      ],
      "metadata": {
        "id": "T8o2BTuX2QHu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction [TODO]\n",
        "\n",
        "Les LLMs sont très utiles pour réaliser plein de tâches mais sont trop gros et assez coûteux à (ré-)entraîner, finetuner et déployer.\n",
        "\n",
        "Il y'a plusieurs manières de compresser un modèle:\n",
        "### **Quantisation**\n",
        "***Avantages*** 👍🏽\\\n",
        "C'est une méthode qui arrondit les poids des modèles à une petite précision.\n",
        "Cela permet de rendre les modèles légers et sans trop de perte de performances. C'est une méthode qui est relativement facile à mettre en oeuvre, et qui donne de bons résultats en pratique.\n",
        "\n",
        "***Incovénients*** 👎🏽\\\n",
        "Le modèle quantisé ne peut pas être entraîné ou finetuné. Il ne sert que pour de l'inférence.\n",
        "\n",
        "### **Distillation**\n",
        "C'est une méthode qui entraîne un petit modèle (student) à matcher les performances du grand modèle de base (teacher)\n",
        "\n",
        "***Avantages*** 👍🏽\n",
        "- Produit souvent des modèles de très bonne qualité qui matchent les performances du modèle de base.\n",
        "- Le petit modèle résultant est utilisable pour le finetuning.\n",
        "\n",
        "**Incovénients** 👎🏽\\\n",
        "Requiert beaucoup de données d'entrînement car les paramètres du student sont initialisés au hasard.\n",
        "\n",
        "### **Pruning**\n",
        "Le pruning consiste à mettre à zéro certaines connexions de neurones, ou des neurones voire des même des couches entières. Ceci permet d'alléger les modèles vu que les valeurs sont nulles. Quand il est possible, on peut même supprimer les paramètres.\n",
        "\n",
        "***Avantages*** 👍🏽\n",
        "- Facile à mettre en oeuvre.\n",
        "- Plétores de librairies qui permettent de le faire. Certaines méthodes sont intégrées nativement dans PyTorch.\n",
        "\n",
        "***Incovénients*** 👎🏽\\\n",
        "Le modèle résultant est souvent \"déstructuré\" par rapport au modèle de base. Pour un transformers par exemple, on peut se retrouver avec un nombre de têtes d'attention différent selon les couches. Ce qui écarte le modèle résultant d'une architecture standard, et cela a des conséquences sur l'efficacité de l'utilisatin des GPUs.\n",
        "\n",
        "# **Low-rank Approximation**\n",
        "C'est une méthode qui consiste à approximer les matrices des couches linéaires à des matrices de dimensions plus petites.\n",
        "\n",
        "***Avantages*** 👍🏽\n",
        "- Facile et peu coûteux à mettre en pratique.\n",
        "- Ne déstructure pas le modèle résultant.\n",
        "- Ne change pas réellement l'architecture du modèle résultant.\n",
        "- Le modèle résultant peut être réentraîné ou finetuné.\n",
        "\n",
        "***Incovénients*** 👎🏽\\\n",
        "Introduit une grosse perte de performance.\n"
      ],
      "metadata": {
        "id": "d2QNAuKGAf2J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Low-Rank Large Language Model"
      ],
      "metadata": {
        "id": "QqEMohjg7b-K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DSvzGTS2MDC"
      },
      "outputs": [],
      "source": []
    }
  ]
}