{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOi0PqApGLEkbgpaLM3EBNw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yaya-sy/BambaraFrenchBitexts/blob/main/LowRankLLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/yaya-sy/speechscorer/blob/main/demo/speechscorer.ipynb)"
      ],
      "metadata": {
        "id": "vNyxPHRy-OYM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Low-Rank LLM for efficient continued pretraining, finetuning and inference\n",
        "\n",
        "ILLUSTRATION [TODO]\n",
        "\n"
      ],
      "metadata": {
        "id": "T8o2BTuX2QHu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction [TODO]\n",
        "\n",
        "Les LLMs sont trÃ¨s utiles pour rÃ©aliser plein de tÃ¢ches mais sont trop gros et assez coÃ»teux Ã  (rÃ©-)entraÃ®ner, finetuner et dÃ©ployer.\n",
        "\n",
        "Il y'a plusieurs maniÃ¨res de compresser un modÃ¨le:\n",
        "### **Quantisation**\n",
        "***Avantages*** ğŸ‘ğŸ½\\\n",
        "C'est une mÃ©thode qui arrondit les poids des modÃ¨les Ã  une petite prÃ©cision.\n",
        "Cela permet de rendre les modÃ¨les lÃ©gers et sans trop de perte de performances. C'est une mÃ©thode qui est relativement facile Ã  mettre en oeuvre, et qui donne de bons rÃ©sultats en pratique.\n",
        "\n",
        "***IncovÃ©nients*** ğŸ‘ğŸ½\\\n",
        "Le modÃ¨le quantisÃ© ne peut pas Ãªtre entraÃ®nÃ© ou finetunÃ©. Il ne sert que pour de l'infÃ©rence.\n",
        "\n",
        "### **Distillation**\n",
        "C'est une mÃ©thode qui entraÃ®ne un petit modÃ¨le (student) Ã  matcher les performances du grand modÃ¨le de base (teacher)\n",
        "\n",
        "***Avantages*** ğŸ‘ğŸ½\n",
        "- Produit souvent des modÃ¨les de trÃ¨s bonne qualitÃ© qui matchent les performances du modÃ¨le de base.\n",
        "- Le petit modÃ¨le rÃ©sultant est utilisable pour le finetuning.\n",
        "\n",
        "**IncovÃ©nients** ğŸ‘ğŸ½\\\n",
        "Requiert beaucoup de donnÃ©es d'entrÃ®nement car les paramÃ¨tres du student sont initialisÃ©s au hasard.\n",
        "\n",
        "### **Pruning**\n",
        "Le pruning consiste Ã  mettre Ã  zÃ©ro certaines connexions de neurones, ou des neurones voire des mÃªme des couches entiÃ¨res. Ceci permet d'allÃ©ger les modÃ¨les vu que les valeurs sont nulles. Quand il est possible, on peut mÃªme supprimer les paramÃ¨tres.\n",
        "\n",
        "***Avantages*** ğŸ‘ğŸ½\n",
        "- Facile Ã  mettre en oeuvre.\n",
        "- PlÃ©tores de librairies qui permettent de le faire. Certaines mÃ©thodes sont intÃ©grÃ©es nativement dans PyTorch.\n",
        "\n",
        "***IncovÃ©nients*** ğŸ‘ğŸ½\\\n",
        "Le modÃ¨le rÃ©sultant est souvent \"dÃ©structurÃ©\" par rapport au modÃ¨le de base. Pour un transformers par exemple, on peut se retrouver avec un nombre de tÃªtes d'attention diffÃ©rent selon les couches. Ce qui Ã©carte le modÃ¨le rÃ©sultant d'une architecture standard, et cela a des consÃ©quences sur l'efficacitÃ© de l'utilisatin des GPUs.\n",
        "\n",
        "# **Low-rank Approximation**\n",
        "C'est une mÃ©thode qui consiste Ã  approximer les matrices des couches linÃ©aires Ã  des matrices de dimensions plus petites.\n",
        "\n",
        "***Avantages*** ğŸ‘ğŸ½\n",
        "- Facile et peu coÃ»teux Ã  mettre en pratique.\n",
        "- Ne dÃ©structure pas le modÃ¨le rÃ©sultant.\n",
        "- Ne change pas rÃ©ellement l'architecture du modÃ¨le rÃ©sultant.\n",
        "- Le modÃ¨le rÃ©sultant peut Ãªtre rÃ©entraÃ®nÃ© ou finetunÃ©.\n",
        "\n",
        "***IncovÃ©nients*** ğŸ‘ğŸ½\\\n",
        "Introduit une grosse perte de performance.\n"
      ],
      "metadata": {
        "id": "d2QNAuKGAf2J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Low-Rank Large Language Model"
      ],
      "metadata": {
        "id": "QqEMohjg7b-K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DSvzGTS2MDC"
      },
      "outputs": [],
      "source": []
    }
  ]
}